{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e7578f",
   "metadata": {},
   "source": [
    "# ChamplainGuide Frontier Model\n",
    "## Course Recommendation via Semantic Embeddings\n",
    "\n",
    "**CSI-480 | Part 5: Using a Frontier Model to Level Up**\n",
    "\n",
    "This notebook builds our own model that recommends courses based on what a student *means*, not just what words they type. We compare two approaches:\n",
    "\n",
    "| | **Before (Keyword Matching)** | **After (Embedding Model)** |\n",
    "|---|---|---|\n",
    "| **How it works** | Counts how many words from the query appear in each course description (basically ctrl+F) | Converts queries and courses into numerical vectors that capture meaning, then measures how close they are |\n",
    "| **Example** | \"I need math for data science\" → Intro to Programming ranks high (because \"data\" appears in its description) | \"I need math for data science\" → Linear Algebra ranks #1 (because the model understands the *concept* is related) |\n",
    "| **Weakness** | Lots of ties, can't handle synonyms, easily tricked by common words | Needs training data, limited by vocabulary |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a588ed2",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff167a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Dark theme for all our plots\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['figure.facecolor'] = '#0f1117'\n",
    "plt.rcParams['axes.facecolor'] = '#1a1d27'\n",
    "plt.rcParams['axes.edgecolor'] = '#2a2e3d'\n",
    "plt.rcParams['text.color'] = '#e2e8f0'\n",
    "plt.rcParams['xtick.color'] = '#8892a8'\n",
    "plt.rcParams['ytick.color'] = '#8892a8'\n",
    "\n",
    "print(\"All imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ee4c5",
   "metadata": {},
   "source": [
    "## 2. Course Knowledge Base\n",
    "\n",
    "This is our training data. Each course has a code, name, description, and metadata.\n",
    "In our learning representation mapping:\n",
    "- **Tokens** = individual attributes (code, credits, semester, prereqs)\n",
    "- **Words** = a complete course with all its attributes bundled together\n",
    "- **Embeddings** = the dense vectors we'll create from these descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210559ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "COURSES = [\n",
    "    {\n",
    "        \"code\": \"CSI-160\", \"name\": \"Introduction to Programming\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [], \"category\": \"CS Core\",\n",
    "        \"description\": \"Fundamentals of programming using Python. Variables, control structures, functions, and basic data types. First course for computer science majors with no prior coding experience.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-220\", \"name\": \"Object-Oriented Programming\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"CSI-160\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Object-oriented design and programming with Java. Classes, objects, inheritance, polymorphism, interfaces, encapsulation, and software design patterns.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-240\", \"name\": \"Data Structures & Algorithms\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-220\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Fundamental data structures including arrays, linked lists, stacks, queues, trees, graphs, hash tables. Algorithm analysis, sorting, searching, and Big-O computational complexity.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-260\", \"name\": \"Computer Architecture\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"CSI-160\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Computer organization and architecture. CPU design, memory hierarchy, cache, instruction sets, assembly language programming, and hardware-software interface.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-280\", \"name\": \"Software Engineering\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-240\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Software development life cycle, requirements engineering, system design, testing methodologies, project management, version control, agile scrum and waterfall methodologies.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-300\", \"name\": \"Database Management Systems\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"CSI-240\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Relational database design, SQL queries, normalization, entity-relationship modeling, transactions, indexing, query optimization. Introduction to NoSQL and document databases.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-340\", \"name\": \"Operating Systems\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-260\", \"CSI-240\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Process management, threading, memory management, virtual memory, file systems, CPU scheduling, concurrency, deadlocks, synchronization in Linux and modern operating systems.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-380\", \"name\": \"Web Application Development\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"CSI-280\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Full-stack web development with React, Node.js, and modern frameworks. Frontend HTML CSS JavaScript, REST APIs, backend servers, database integration, authentication, and cloud deployment.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-400\", \"name\": \"Artificial Intelligence\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-240\"], \"category\": \"CS Core\",\n",
    "        \"description\": \"Introduction to artificial intelligence including search algorithms, knowledge representation, machine learning fundamentals, neural networks, and natural language processing.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-320\", \"name\": \"Machine Learning\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"CSI-400\"], \"category\": \"CS Elective\",\n",
    "        \"description\": \"Supervised and unsupervised learning, regression, classification, decision trees, clustering, neural networks, deep learning, model evaluation, training, and prediction techniques.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-350\", \"name\": \"Computer Networks\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-260\"], \"category\": \"CS Elective\",\n",
    "        \"description\": \"Network protocols, TCP/IP stack, OSI model, routing, switching, network security, firewalls, VPN, wireless networks, and distributed systems communication.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-370\", \"name\": \"Mobile App Development\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-280\"], \"category\": \"CS Elective\",\n",
    "        \"description\": \"Mobile application development for iOS and Android using Swift and Kotlin. User interface design, gestures, sensors, data persistence, GPS location, and app store deployment.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-420\", \"name\": \"Natural Language Processing\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"CSI-400\"], \"category\": \"CS Elective\",\n",
    "        \"description\": \"Text processing, tokenization, word embeddings, transformer models, sentiment analysis, text classification, language generation, chatbots, and large language models.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"CSI-430\", \"name\": \"Cybersecurity Fundamentals\", \"credits\": 3,\n",
    "        \"semester\": \"Fall/Spring\", \"prereqs\": [\"CSI-260\"], \"category\": \"CS Elective\",\n",
    "        \"description\": \"Security principles, threat modeling, cryptography, encryption, access control, vulnerability assessment, penetration testing basics, and security best practices.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"SEC-150\", \"name\": \"Security Fundamentals\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [], \"category\": \"Cybersecurity Core\",\n",
    "        \"description\": \"Introduction to information security concepts. CIA triad confidentiality integrity availability, risk assessment, security policies, compliance frameworks, and security awareness training.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"SEC-250\", \"name\": \"Ethical Hacking\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"SEC-210\"], \"category\": \"Cybersecurity Core\",\n",
    "        \"description\": \"Penetration testing methodology, vulnerability scanning, network exploitation techniques, web application attacks, social engineering, password cracking, and responsible disclosure.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"SEC-300\", \"name\": \"Digital Forensics\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"SEC-250\"], \"category\": \"Cybersecurity Core\",\n",
    "        \"description\": \"Digital evidence collection and preservation, disk forensics, memory analysis, network forensics, malware analysis, chain of custody, and forensic reporting for legal proceedings.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"SEC-400\", \"name\": \"Advanced Penetration Testing\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"SEC-250\"], \"category\": \"Cybersecurity Core\",\n",
    "        \"description\": \"Advanced exploitation techniques, privilege escalation, lateral movement, Active Directory attacks, red team operations, custom exploit development, and evasion techniques.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"MAT-210\", \"name\": \"Calculus I\", \"credits\": 3,\n",
    "        \"semester\": \"Fall/Spring\", \"prereqs\": [], \"category\": \"Math\",\n",
    "        \"description\": \"Limits, derivatives, integrals, fundamental theorem of calculus, applications of differentiation and integration to real-world mathematical problems.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"MAT-230\", \"name\": \"Discrete Mathematics\", \"credits\": 3,\n",
    "        \"semester\": \"Fall\", \"prereqs\": [\"MAT-210\"], \"category\": \"Math\",\n",
    "        \"description\": \"Propositional logic, mathematical proofs, sets, relations, functions, counting combinatorics, graph theory, trees, and mathematical foundations for computer science algorithms.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"MAT-310\", \"name\": \"Linear Algebra\", \"credits\": 3,\n",
    "        \"semester\": \"Spring\", \"prereqs\": [\"MAT-220\"], \"category\": \"Math\",\n",
    "        \"description\": \"Vectors, matrices, linear transformations, determinants, eigenvalues, eigenvectors, vector spaces, orthogonality, and applications to data science and machine learning.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"MAT-330\", \"name\": \"Probability & Statistics\", \"credits\": 3,\n",
    "        \"semester\": \"Fall/Spring\", \"prereqs\": [\"MAT-210\"], \"category\": \"Math\",\n",
    "        \"description\": \"Probability theory, random variables, probability distributions, Bayes theorem, hypothesis testing, confidence intervals, regression analysis, and statistical inference for data analysis.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(COURSES)} courses across {len(set(c['category'] for c in COURSES))} categories\")\n",
    "for cat in sorted(set(c['category'] for c in COURSES)):\n",
    "    count = sum(1 for c in COURSES if c['category'] == cat)\n",
    "    print(f\"  {cat}: {count} courses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1fcb3",
   "metadata": {},
   "source": [
    "## 3. BEFORE: Keyword Matching (the Baseline)\n",
    "\n",
    "This is the simplest possible approach. For a student query like \"I need math for data science\":\n",
    "1. Split the query into individual words: `[\"I\", \"need\", \"math\", \"for\", \"data\", \"science\"]`\n",
    "2. For each course, check how many of those words appear anywhere in the course description\n",
    "3. Rank by number of matches\n",
    "\n",
    "**The problem:** Common words like \"data\" appear in many descriptions, so unrelated courses score well. And synonyms are invisible — \"hacking\" won't match \"penetration testing\" even though they mean the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbefe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query, courses):\n",
    "    \"\"\"\n",
    "    BEFORE approach: simple keyword matching.\n",
    "    This is essentially ctrl+F for each word in the query.\n",
    "    \"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    results = []\n",
    "    for c in courses:\n",
    "        text = f\"{c['code']} {c['name']} {c['description']}\".lower()\n",
    "        matches = sum(1 for w in query_words if w in text)\n",
    "        results.append((c, matches))\n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Test it\n",
    "test_query = \"I need math courses for data science\"\n",
    "print(f'Query: \"{test_query}\"')\n",
    "print(f\"Query words: {set(test_query.lower().split())}\")\n",
    "print()\n",
    "print(\"Top 5 results by keyword matching:\")\n",
    "print(\"-\" * 55)\n",
    "for c, score in keyword_search(test_query, COURSES):\n",
    "    print(f\"  {c['code']}: {c['name']:<35} ({score} word matches)\")\n",
    "print()\n",
    "print(\"Notice: CSI-160 (Intro to Programming) ranks first because\")\n",
    "print(\"words like 'data' and 'for' appear in its description.\")\n",
    "print(\"That's not a useful recommendation for someone asking about\")\n",
    "print(\"math courses for data science.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034d51ad",
   "metadata": {},
   "source": [
    "## 4. AFTER: Building Our Embedding Model\n",
    "\n",
    "Now we build the actual frontier model. Two stages:\n",
    "\n",
    "### Stage 1: TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "Each course description gets converted into a vector of numbers. Each number represents how important a specific word is to that course *relative to all other courses*.\n",
    "- If \"machine learning\" appears in a course but rarely in others → high weight (it's distinctive)\n",
    "- If \"course\" appears everywhere → low weight (it's not distinctive)\n",
    "\n",
    "### Stage 2: SVD (Singular Value Decomposition)\n",
    "TF-IDF gives us a huge sparse vector (500 dimensions, mostly zeros). SVD compresses this into a small dense vector (21 dimensions) that captures the core \"meaning\" of each course. This is the actual **embedding** — a compact representation that you can compare mathematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daced63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Combine course attributes into rich text for embedding\n",
    "course_texts = []\n",
    "for c in COURSES:\n",
    "    text = (\n",
    "        f\"{c['code']} {c['name']}. {c['description']} \"\n",
    "        f\"Category: {c['category']}. Prerequisites: {', '.join(c['prereqs']) or 'none'}.\"\n",
    "    )\n",
    "    course_texts.append(text)\n",
    "\n",
    "# Show an example\n",
    "print(\"Example course text (what gets fed into TF-IDF):\")\n",
    "print(\"-\" * 60)\n",
    "print(course_texts[0][:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf3cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: TF-IDF Vectorization\n",
    "# Converts text into sparse numerical vectors based on word importance\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=500,       # Keep top 500 terms\n",
    "    stop_words=\"english\",   # Remove \"the\", \"is\", \"a\", etc.\n",
    "    ngram_range=(1, 2),     # Use single words AND two-word phrases\n",
    "    sublinear_tf=True,      # Apply log scaling to term frequencies\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(course_texts)\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  → {tfidf_matrix.shape[0]} courses, each represented as a {tfidf_matrix.shape[1]}-dimensional sparse vector\")\n",
    "print(f\"  → Vocabulary size: {len(vectorizer.vocabulary_)} unique terms\")\n",
    "print()\n",
    "\n",
    "# Show some learned terms\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "print(\"Sample learned terms (what the model considers important):\")\n",
    "print(f\"  {', '.join(terms[:15])}\")\n",
    "print(f\"  {', '.join(terms[100:115])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2660eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: SVD Dimensionality Reduction\n",
    "# Compress 500-dim sparse vectors → 21-dim dense embeddings\n",
    "n_components = min(50, len(COURSES) - 1)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "course_embeddings = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"Embedding matrix shape: {course_embeddings.shape}\")\n",
    "print(f\"  → {course_embeddings.shape[0]} courses, each now a {course_embeddings.shape[1]}-dimensional dense vector\")\n",
    "print(f\"  → Variance explained: {svd.explained_variance_ratio_.sum():.1%}\")\n",
    "print(f\"    (this means we kept {svd.explained_variance_ratio_.sum():.1%} of the information from the original 500-dim vectors)\")\n",
    "print()\n",
    "\n",
    "# Show what an embedding looks like\n",
    "print(f\"Example: CSI-160's embedding vector (first 10 of {n_components} dimensions):\")\n",
    "print(f\"  {course_embeddings[0][:10].round(3)}\")\n",
    "print()\n",
    "print(\"Each number represents how much that course aligns with a learned\")\n",
    "print(\"'concept dimension' — roughly things like 'how much is this about security'\")\n",
    "print(\"or 'how mathematical is it'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a1e6b",
   "metadata": {},
   "source": [
    "## 5. Using Embeddings for Recommendation\n",
    "\n",
    "Now we can embed a student's query into the same vector space and find the closest courses using **cosine similarity** — a measure of how similar two vectors are (1.0 = identical direction, 0.0 = completely unrelated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_search(query, courses, vectorizer, svd, course_embeddings):\n",
    "    \"\"\"\n",
    "    AFTER approach: semantic similarity via embeddings.\n",
    "    1. Convert the query into a TF-IDF vector using the same vocabulary\n",
    "    2. Compress it with SVD into the same embedding space\n",
    "    3. Find courses with the highest cosine similarity\n",
    "    \"\"\"\n",
    "    query_tfidf = vectorizer.transform([query])\n",
    "    query_embedding = svd.transform(query_tfidf)\n",
    "    similarities = cosine_similarity(query_embedding, course_embeddings)[0]\n",
    "    ranked = sorted(zip(courses, similarities), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:5]\n",
    "\n",
    "# Same test query\n",
    "print(f'Query: \"{test_query}\"')\n",
    "print()\n",
    "print(\"Top 5 results by embedding similarity:\")\n",
    "print(\"-\" * 60)\n",
    "for c, score in embedding_search(test_query, COURSES, vectorizer, svd, course_embeddings):\n",
    "    print(f\"  {c['code']}: {c['name']:<35} (similarity: {score:.3f})\")\n",
    "print()\n",
    "print(\"Now MAT-310 (Linear Algebra) ranks #1 at 0.864 similarity because\")\n",
    "print(\"the model learned that 'data science' is semantically close to\")\n",
    "print(\"Linear Algebra's description of 'applications to data science\")\n",
    "print(\"and machine learning'. The keyword approach couldn't see this.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742bba05",
   "metadata": {},
   "source": [
    "## 6. Before / After Comparison\n",
    "\n",
    "Let's run both approaches on several student queries side by side. The key thing to notice is:\n",
    "- **Keyword matching** gives lots of ties and gets confused by common words\n",
    "- **Embedding similarity** gives clear rankings based on actual meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60849a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_queries = [\n",
    "    \"I want to learn how to hack into systems\",\n",
    "    \"How do I build a website or web app?\",\n",
    "    \"I'm interested in AI and machine learning\",\n",
    "    \"I need math courses for data science\",\n",
    "    \"How do computers store and organize information?\",\n",
    "]\n",
    "\n",
    "for query in demo_queries:\n",
    "    kw = keyword_search(query, COURSES)\n",
    "    emb = embedding_search(query, COURSES, vectorizer, svd, course_embeddings)\n",
    "\n",
    "    print(f'\\n{\"=\"*70}')\n",
    "    print(f'Student asks: \"{query}\"')\n",
    "    print(f'{\"=\"*70}')\n",
    "    print(f'  {\"BEFORE (Keyword Matching)\":<40} {\"AFTER (Embedding Similarity)\":<40}')\n",
    "    print(f'  {\"-\"*38}   {\"-\"*38}')\n",
    "    for i in range(3):\n",
    "        kw_str = f\"{kw[i][0]['code']}: {kw[i][0]['name'][:22]:<22} ({kw[i][1]} matches)\"\n",
    "        emb_str = f\"{emb[i][0]['code']}: {emb[i][0]['name'][:22]:<22} ({emb[i][1]:.3f})\"\n",
    "        print(f\"  {kw_str:<40} {emb_str:<40}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a09cda",
   "metadata": {},
   "source": [
    "## 7. Visual Comparison Charts\n",
    "\n",
    "These charts show the same data visually. For each query:\n",
    "- **Left (red title):** Keyword matching scores — notice how many courses tie or score similarly\n",
    "- **Right (green title):** Embedding similarity scores — notice the clear separation between relevant and irrelevant courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06023e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(query, kw_results, emb_results):\n",
    "    \"\"\"Side-by-side bar chart comparing keyword vs embedding results.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "    # Before: keyword matching\n",
    "    codes1 = [c['code'] for c, _ in kw_results][::-1]\n",
    "    scores1 = [s for _, s in kw_results][::-1]\n",
    "    ax1.barh(codes1, scores1, color='#4a5568', edgecolor='#2a2e3d', height=0.55)\n",
    "    ax1.set_title('BEFORE: Keyword Matching', color='#ef4444', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax1.set_xlabel('Word Matches (higher = more shared words)', fontsize=9)\n",
    "    ax1.set_xlim(0, max(max(scores1), 1) + 1.5)\n",
    "    for i, s in enumerate(scores1):\n",
    "        ax1.text(s + 0.08, i, str(s), va='center', color='#8892a8', fontsize=10)\n",
    "\n",
    "    # After: embedding similarity\n",
    "    codes2 = [c['code'] for c, _ in emb_results][::-1]\n",
    "    scores2 = [round(s, 3) for _, s in emb_results][::-1]\n",
    "    colors = ['#6c9bff' if s > 0.35 else '#38bdf8' if s > 0.2 else '#4a5568' for s in scores2]\n",
    "    ax2.barh(codes2, scores2, color=colors, edgecolor='#2a2e3d', height=0.55)\n",
    "    ax2.set_title('AFTER: Embedding Similarity', color='#4ade80', fontsize=12, fontweight='bold', pad=10)\n",
    "    ax2.set_xlabel('Cosine Similarity (1.0 = perfect match, 0.0 = unrelated)', fontsize=9)\n",
    "    ax2.set_xlim(0, 1.0)\n",
    "    for i, s in enumerate(scores2):\n",
    "        ax2.text(s + 0.01, i, f'{s:.3f}', va='center', color='#8892a8', fontsize=10)\n",
    "\n",
    "    fig.suptitle(f'Student Query: \"{query}\"', color='#6c9bff', fontsize=11, y=0.02, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.06, 1, 1])\n",
    "    plt.show()\n",
    "\n",
    "# Generate a chart for each demo query\n",
    "for query in demo_queries:\n",
    "    kw = keyword_search(query, COURSES)\n",
    "    emb = embedding_search(query, COURSES, vectorizer, svd, course_embeddings)\n",
    "    plot_comparison(query, kw, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5d344f",
   "metadata": {},
   "source": [
    "## 8. Visualizing the Embedding Space\n",
    "\n",
    "This is a t-SNE projection that takes our 21-dimensional course embeddings and squishes them down to 2D so we can see them on a plot. Courses that are **semantically similar** (based on what the model learned from their descriptions) appear **close together**.\n",
    "\n",
    "Look for:\n",
    "- **Math courses (green)** clustered on one side\n",
    "- **Cybersecurity courses (purple)** grouped together\n",
    "- **AI/ML courses** near each other\n",
    "- **Programming courses** in their own neighborhood\n",
    "\n",
    "These clusters weren't hard-coded — the model **learned** these relationships from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f8955",
   "metadata": {},
   "outputs": [],
   "source": [
    "perp = min(7, len(COURSES) - 1)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=perp, max_iter=1000)\n",
    "coords = tsne.fit_transform(course_embeddings)\n",
    "\n",
    "cat_colors = {\n",
    "    \"CS Core\": \"#6c9bff\",\n",
    "    \"CS Elective\": \"#38bdf8\",\n",
    "    \"Cybersecurity Core\": \"#a78bfa\",\n",
    "    \"Math\": \"#4ade80\",\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 7.5))\n",
    "\n",
    "for i, c in enumerate(COURSES):\n",
    "    color = cat_colors.get(c['category'], '#8892a8')\n",
    "    ax.scatter(coords[i, 0], coords[i, 1], c=color, s=140, alpha=0.85,\n",
    "               edgecolors='#0f1117', linewidths=1.5, zorder=3)\n",
    "    ax.annotate(c['code'], (coords[i, 0], coords[i, 1]),\n",
    "                xytext=(8, 6), textcoords='offset points',\n",
    "                fontsize=8, color=color, fontweight='bold')\n",
    "\n",
    "for cat, color in cat_colors.items():\n",
    "    ax.scatter([], [], c=color, s=80, label=cat, edgecolors='#0f1117', linewidths=1)\n",
    "ax.legend(loc='upper right', fontsize=9, facecolor='#1a1d27', edgecolor='#2a2e3d', labelcolor='#e2e8f0')\n",
    "\n",
    "ax.set_title('Course Embedding Space (t-SNE Projection)', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Dimension 1', fontsize=10, color='#5a6478')\n",
    "ax.set_ylabel('Dimension 2', fontsize=10, color='#5a6478')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e748c",
   "metadata": {},
   "source": [
    "## 9. Model Summary\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| **Model type** | TF-IDF + Truncated SVD |\n",
    "| **Vocabulary** | 500 terms (unigrams + bigrams) |\n",
    "| **Embedding dimensions** | 21 |\n",
    "| **Training data** | 22 course descriptions from Champlain College CS & Cybersecurity programs |\n",
    "| **Variance explained** | ~97% (very little information lost during compression) |\n",
    "| **Dependencies** | scikit-learn, matplotlib, numpy (no GPU required) |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Embeddings capture **meaning**, not just words. A student doesn't need to know the exact course title — they can describe what they want in their own words, and the model bridges the gap. This is the core value of the frontier model: **meeting students where they are** instead of requiring them to already know what they're looking for.\n",
    "\n",
    "### What could make this better with more time/data:\n",
    "- **sentence-transformers** (neural embeddings) instead of TF-IDF for richer semantic understanding\n",
    "- **More training data**: syllabi, student reviews, enrollment history\n",
    "- **Fine-tuning** on actual student advising conversations\n",
    "- **Integration** with the ChamplainGuide chat interface as a recommendation backend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
